<p align="center">
  <img width="100%" src="https://i.imgur.com/tm9mSuM.png"/>
</p>

# Prompt Engineeringâ€™s Influence on Result Reliability in Large Language Models

## Abstract

Large Language Models (LLMs) have become integral components in various applications, ranging from natural language understanding to content generation. As their significance grows, so does the need to scrutinize and evaluate their responses due to potential risks such as private data leaks and the generation of inappropriate, harmful, or misleading content.

This master's thesis delves into the assessment of Large Language Models responses, introducing key metrics such as correctness, coherence, relevance, and completeness. Recognizing that the quality of responses is intricately linked to the nature of the questions posed to Large Language Models, we explore the concept of prompt engineering as a crucial factor influencing response quality.

To conduct a comprehensive evaluation, we utilize multi-domain datasets as well as domain-specific ones, encompassing both open-answers and multiple-choice formats, in order to ensure an unbiased and generalized evaluation. 
Our analysis reveals intriguing findings, demonstrating that the effectiveness of prompt engineering is not universal. While in certain cases, optimizing prompts enhances response quality, in others, it introduces unnecessary complexity, leading to inferior results.

To learn more about the research check out the [Executive Summary](2024_04_della%20Volpe_Executive%20Summary_02.pdf).

## Table of Contents

- [Statement of the Problem](#statement-of-the-problem)
- [Research Objectives](#research-objectives)
- [Methodology Workflow](#methodology-workflow)
- [Experiment Notebooks](#experiment-notebooks)
- [Datasets](#datasets)
- [Contributors](#contributors)

## Statement of the Problem

In today's fast-changing world of computer engineering, Large Language Models (LLMs) have emerged as powerful tools used in various applications. They excel in understanding natural language and generating content. Their importance is evident across different fields, from language comprehension to content creation. However, as LLMs become more widespread, it's crucial to scrutinize their outputs to address potential risks such as data privacy breaches and the creation of inappropriate or misleading content.

Our research focuses on understanding the challenges that come with using LLMs. As these models become more widely used, there's a risk of private data leaks and the creation of harmful or misleading content. This requires us to thoroughly examine the quality of LLM responses, setting the stage for our exploration into the intricate interplay between these models and the questions posed to them.

## Research Objectives

The key objectives of this master's thesis are to:

- Evaluate the reliability of Large Language Models (LLMs)
- Discern the conditions under which prompt engineering strategies prove effective or counterproductive.

## Methodology Workflow

Our methodology consists of the following key steps:

1. **Dataset Selection and Preparation:**
    - Begin by selecting diverse datasets relevant to our research objectives. We employ datasets of different types (Multiple-Choice Question Answer and Open-Ended Answer) and both domain-specific or multi-domain, to ensure unbiased and universal results.
    - Prepare the datasets by ensuring they are formatted appropriately for analysis.

2. **Definition of Key Metrics:**
    - We introduce and define key metrics, including correctness, coherence, relevance, and completeness. These metrics serve as the foundation for assessing the quality of responses generated by LLMs.

3. **Question Extraction:**
    - From each dataset, extract a subset of questions for evaluation. Aim for a consistent number, such as 50 questions per dataset.
    
4. **Prompt Engineering:**
    - We investigate various prompt engineering strategies, such as Thought Chaining, Self Correction, Context Injection, and Tagged Source. These strategies are systematically applied to specific datasets in an iterative manner. Our approach aims to prompt the model to generate responses tailored to the unique characteristics of the questions posed.

5. **LLM Evaluation:**
    - Present both the original questions and the engineered variations to the LLM (in this case, GPT-3.5 Turbo).
    - Collect responses from the LLM for each question.

6. **Evaluation Criteria:**
    - For Multiple Choice Question Answering (MCQA) datasets:
        - Evaluate correctness by comparing the option index provided by the model to the correct option in the ground truth dataset.
    - For Open-Ended Answer datasets:
        - Evaluate completeness, coherence, and relevance of the free-text responses generated by the LLM.

7. **Result Analysis:**
    - Analyze the responses obtained from the LLM across different prompt engineering strategies.
    - Assess the effectiveness of each strategy in improving response quality.
    - Interpret the results to identify trends, patterns, and insights regarding the performance of the LLM.

### Experiment Notebooks:

The colab notebooks for running experiments, analyzing results, and visualizing data related to prompt engineering and response evaluation in Large Language Models.

- [Evaluation of Correctness Race - high.ipynb](Evaluation%20of%20Correctness%20Race%20-%20high.ipynb)
- [Evaluation of Correctness Race - middle.ipynb](Evaluation%20of%20Correctness%20Race%20-%20middle.ipynb)
- [Evaluation of Correctness math_qa.ipynb](Evaluation%20of%20Correctness%20math_qa.ipynb)
- [Evaluation of Correctness medmcqa.ipynb](Evaluation%20of%20Correctness%20medmcqa.ipynb)
- [Evaluation of Text Quality Measures MedQuad-MedicalQnA.ipynb](Evaluation%20of%20Text%20Quality%20Measures%20MedQuad-MedicalQnA.ipynb)
- [Evaluation of Text Quality Measures MultiContextLongAnswer.ipynb](Evaluation%20of%20Text%20Quality%20Measures%20MultiContextLongAnswer.ipynb)

The experiments conducted in this study leverage various libraries, frameworks, and APIs, creating a comprehensive technological environment for the development and evaluation of language models. The key technologies employed are as follows:

- **Google Colab:** Google Colab is a free, cloud-based platform designed for writing and executing Python code. It offers a convenient environment, particularly well-suited for running experiments, including those related to machine learning.

- **OpenAI GPT-3.5-turbo:** GPT-3.5-turbo, developed by OpenAI, is a prominent language model within the GPT series. Renowned for its natural language processing capabilities, it serves as a crucial component in the experiments conducted.

- **Python Libraries:**
  - **datasets:** Used for loading and working with datasets.
  - **pandas:** Employed for data manipulation and analysis.
  - **random:** Utilized for generating random numbers.
  - **openai:** OpenAI's Python library facilitates interactions with the GPT-3.5-turbo API.
  - **matplotlib:** A comprehensive plotting library for creating visualizations in Python.

- **Machine Learning Models:**
  - **SBERT (Sentence-BERT):** SBERT plays an indirect role in calculating relevance scores. It is a variation of BERT, a pre-trained language model developed by Google, specifically adapted for computing dense sentence embeddings, thereby facilitating semantic similarity calculations in natural language processing tasks.

- **Additional Tools:**
  - **CountVectorizer and cosine similarity (from scikit-learn):** Utilized to calculate coherence scores by measuring the similarity between generated and ground truth answers.

## Datasets
 
- ### [medMCQA (Medical Multiple Choice Question-Answer) Dataset](https://huggingface.co/datasets/medmcqa) [Medical | Multiple Choice]

- ### [mathQA (Mathematics Question-Answer) Dataset](https://huggingface.co/datasets/math_qa) [Mathematics | Multiple Choice]

- ### [RACE (Reading Comprehension from Examinations) Dataset](https://huggingface.co/datasets/race) [General Knowledge | Multiple Choice]

- ### [Multi-Context Long Answer Dataset](https://huggingface.co/datasets/nbtpj/multi-context-long-answer-dataset) [General Knowledge | Open Answer]

- ### [MedQuad-MedicalQnA Dataset](https://huggingface.co/datasets/keivalya/MedQuad-MedicalQnADataset) [Medical | Open Answer]

## Contributors

- Author: Nicola della Volpe
- Advisor: Prof. Cinzia Cappiello

## License

This project is licensed under the [MIT License](LICENSE).
